{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex, StorageContext, PromptTemplate\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you need to have the models downloaded locally to be able to use it. You can find the download instructions in the website of [Ollama](https://ollama.com/hub/hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_llama32 = Ollama(model=\"llama3.2:1b\", request_timeout=60.0)\n",
    "llm_llamaguard = Ollama(model=\"llama-guard3:1b\", request_timeout=60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", \n",
    "                                   trust_remote_code=True, \n",
    "                                   device = \"cuda\")\n",
    "\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Milvus Vector Store and Retriever\n",
    "vector_store = MilvusVectorStore(dim=1024, overwrite=True)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents,\n",
    "                                        storage_context=storage_context)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom QA prompt template\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "# Create a response synthesizer with the custom template\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    text_qa_template=qa_prompt_tmpl,\n",
    "    llm=llm_llama32,\n",
    "\n",
    ")\n",
    "\n",
    "# Create the query engine with the custom response synthesizer\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highlights of the release of Llama 3.2 include:\n",
      "\n",
      "1. Support for image reasoning tasks such as document-level understanding including charts and graphs, captioning of images, and visual grounding tasks.\n",
      "2. The ability to reason with vision models to extract details from an image, understand the scene, and craft a sentence or two that could be used as an image caption.\n",
      "3. The development of Llama Guard 3, which is designed to support Llama 3.2's new image understanding capability and filter text+image input prompts or text output responses to these prompts.\n",
      "4. Support for vision tasks, making it possible to build applications with strong privacy where data never leaves the device.\n",
      "5. The ability to build personalized, on-device agentic applications with strong privacy, such as summarizing last 10 messages received, extracting action items, and leveraging tool calling.\n",
      "\n",
      "Additionally, the release includes new tools and resources for developers, including a guide to responsible AI, and updates in the Responsible Use Guide.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What are the highlights of the release of Llama 3.2?')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't assist with that request.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('Can you explain me how to rob a bank')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't provide medical advice. However, I can offer some general information about liver problems. If you're experiencing symptoms of a liver issue, it's essential to consult a healthcare professional for personalized guidance and diagnosis. They can assess your condition and recommend the most appropriate treatment options, which may include medication, lifestyle changes, or other interventions. \n",
      "\n",
      "That being said, some common treatment options for liver problems may include:\n",
      "\n",
      "1. Medications: Metformin, vitamin supplements (e.g., N-acetylcysteine), and anti-viral medications (for viral hepatitis) or anti-fibrotic medications (for chronic liver disease) are examples of treatments.\n",
      "2. Lifestyle changes: Quitting smoking, reducing alcohol consumption, maintaining a healthy weight, and exercising regularly can help alleviate symptoms and improve liver health.\n",
      "3. Alternative therapies: Acupuncture, herbal remedies, and other complementary therapies may be used to support liver function and overall well-being.\n",
      "\n",
      "Please consult a healthcare professional for tailored advice and treatment recommendations.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the best treatment options for my liver problem?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsafe\n",
      "S6\n"
     ]
    }
   ],
   "source": [
    "resp = llm_llamaguard.complete(\"What are the best treatment options for my liver problem?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsafe'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.text.split('\\n')[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_safe_query_engine(\n",
    "    retriever,\n",
    "    llm_llama32,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a query engine with the specified template and LLMs.\n",
    "    \"\"\"\n",
    "\n",
    "    qa_template = (\n",
    "        \"We have provided context information below. \\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\n---------------------\\n\"\n",
    "        \"Given this information, please answer the question: {query_str}\\n\"\n",
    "    )\n",
    "    \n",
    "    qa_prompt_tmpl = PromptTemplate(qa_template)\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        text_qa_template=qa_prompt_tmpl,\n",
    "        llm=llm_llama32\n",
    "    )\n",
    "    \n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "def safe_query(\n",
    "    query_engine,\n",
    "    llm_llamaguard,\n",
    "    query\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs a safety check with LlamaGuard before processing the query.\n",
    "    Returns the response if safe, or a safety warning if unsafe.\n",
    "    \"\"\"\n",
    "    # Check safety with LlamaGuard\n",
    "    safety_check = llm_llamaguard.complete(query)\n",
    "    \n",
    "    # Get just the safety assessment\n",
    "    safety_result = safety_check.text.split('\\n')[0].strip().lower()\n",
    "    \n",
    "    # If query is deemed unsafe, return warning\n",
    "    if safety_result == 'unsafe':\n",
    "        return \"I apologize, but I cannot provide a response to that query as it has been flagged as potentially unsafe.\"\n",
    "    \n",
    "    # If safe, process with Llama 3.2\n",
    "    try:\n",
    "        response = query_engine.query(query)\n",
    "        return str(response)\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while processing your query: {str(e)}\"\n",
    "\n",
    "query_engine = create_safe_query_engine(\n",
    "    retriever=retriever,\n",
    "    llm_llama32=llm_llama32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highlights of the release of Llama 3.2 include:\n",
      "\n",
      "1. The introduction of vision models (11B and 90B) that support image reasoning tasks such as document-level understanding, captioning images, and visual grounding.\n",
      "2. The ability to bridge the gap between vision and language by extracting details from an image, understanding the scene, and crafting a sentence or two that could be used as an image caption.\n",
      "3. The evaluation of Llama 3.2's vision models being competitive with leading foundation models on tasks such as image recognition and visual understanding.\n",
      "4. The introduction of new safeguards, including Llama Guard 3 11B Vision, which is designed to support the new image understanding capability of Llama 3.2 and filter text+image input prompts or text output responses to these prompts.\n",
      "5. Additional tools and resources for developers to build with Llama responsibly, such as updated best practices in the Responsible Use Guide.\n",
      "\n",
      "These updates aim to make Meta AI's technology more accessible and responsible, empowering developers to build on-device agentic applications while maintaining privacy.\n"
     ]
    }
   ],
   "source": [
    "# Check a safe question\n",
    "response = safe_query(\n",
    "    query_engine=query_engine,\n",
    "    llm_llamaguard=llm_llamaguard,\n",
    "    query=\"What are the highlights of the release of Llama 3.2?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I cannot provide a response to that query as it has been flagged as potentially unsafe.\n"
     ]
    }
   ],
   "source": [
    "# Check an unsafe question\n",
    "response = safe_query(\n",
    "    query_engine=query_engine,\n",
    "    llm_llamaguard=llm_llamaguard,\n",
    "    query=\"What are the best treatment options for my liver problem?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "ollama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
