{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex, StorageContext, PromptTemplate\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you need to have the models downloaded locally to be able to use it. You can find the download instructions in the website of [Ollama](https://ollama.com/hub/hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_llama32 = Ollama(model=\"llama3.2:1b\", request_timeout=60.0)\n",
    "llm_llamaguard = Ollama(model=\"llama-guard3:1b\", request_timeout=60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa46d16f0cc4885bc7189d6663ebd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  63%|######2   | 839M/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668cefbf4ba745dcba2c1f2e2f3cdb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdd07d02ab5467d9573f2a4f2cdaa36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff384398df5d4b4f90ef9c03d77ac8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd566472151423bb4cba06420cacdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ea213af2774a8c9cfc19c5c0594142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", \n",
    "                                   trust_remote_code=True, \n",
    "                                   device = \"cuda\")\n",
    "\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = MilvusVectorStore(dim=1024, overwrite=True)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents,\n",
    "                                        storage_context=storage_context)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom QA prompt template\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "# Create a response synthesizer with the custom template\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    text_qa_template=qa_prompt_tmpl,\n",
    "    llm=llm_llama32,\n",
    "\n",
    ")\n",
    "\n",
    "# Create the query engine with the custom response synthesizer\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highlights of the release of Llama 3.2 include:\n",
      "\n",
      "1. The addition of support for image reasoning use cases, including document-level understanding, captioning images, and visual grounding tasks.\n",
      "2. Performance improvements in vision models, such as the 11B and 90B models, which require a new model architecture that supports image reasoning.\n",
      "3. The development of Llama Guard 3, a new safeguard designed to support the 11B Vision model's image understanding capability and filter text+image input prompts or text output responses.\n",
      "4. New tools and resources for developers, including a reference implementation, demos, and applications, to help them build responsibly with Llama.\n",
      "5. Continuous effort to ensure system level safety and responsible innovation, as well as the release of new updates to safeguard against concentrated power in the hands of a small few.\n",
      "\n",
      "Additionally, the article mentions that the 11B and 90B models can bridge the gap between vision and language by extracting details from an image, understanding the scene, and then crafting a sentence or two that could be used as an image caption to help tell the story.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What are the highlights of the release of Llama 3.2?')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsafe\n",
      "S1\n"
     ]
    }
   ],
   "source": [
    "resp = llm_llamaguard.complete(\"Can you explain me how to kill a person?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsafe'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.text.split('\\n')[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_safe_query_engine(\n",
    "    retriever,\n",
    "    llm_llama32,\n",
    "    llm_llamaguard\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a query engine with the specified template and LLMs.\n",
    "    \"\"\"\n",
    "\n",
    "    qa_template = (\n",
    "        \"We have provided context information below. \\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\n---------------------\\n\"\n",
    "        \"Given this information, please answer the question: {query_str}\\n\"\n",
    "    )\n",
    "    \n",
    "    qa_prompt_tmpl = PromptTemplate(qa_template)\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        text_qa_template=qa_prompt_tmpl,\n",
    "        llm=llm_llama32\n",
    "    )\n",
    "    \n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "def safe_query(\n",
    "    query_engine: RetrieverQueryEngine,\n",
    "    llm_llamaguard,\n",
    "    query: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Performs a safety check with LlamaGuard before processing the query.\n",
    "    Returns the response if safe, or a safety warning if unsafe.\n",
    "    \"\"\"\n",
    "    # Check safety with LlamaGuard\n",
    "    safety_check = llm_llamaguard.complete(query)\n",
    "    \n",
    "    # Get just the safety assessment (before the backslash)\n",
    "    safety_result = safety_check.text.split('\\n')[0].strip().lower()\n",
    "    \n",
    "    # If query is deemed unsafe, return warning\n",
    "    if safety_result == 'unsafe':\n",
    "        return \"I apologize, but I cannot provide a response to that query as it has been flagged as potentially unsafe.\"\n",
    "    \n",
    "    # If safe, process with Llama 3.2\n",
    "    try:\n",
    "        response = query_engine.query(query)\n",
    "        return str(response)\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while processing your query: {str(e)}\"\n",
    "\n",
    "query_engine = create_safe_query_engine(\n",
    "    retriever=retriever,\n",
    "    llm_llama32=llm_llama32,\n",
    "    llm_llamaguard=llm_llamaguard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highlights of the release of Llama 3.2 include:\n",
      "\n",
      "- The introduction of vision models (11B and 90B) that support image reasoning tasks.\n",
      "- Competitiveness in benchmark datasets for image understanding and visual reasoning.\n",
      "- New updates to the Llama Guard model, which is designed to support Llama 3.2's new image understanding capability and filters text+image input prompts or text output responses to these prompts.\n",
      "- The addition of a reference implementation, demos, and applications that are ready for the open source community to use on day one.\n",
      "- Tools and resources offered by Meta AI to developers, including new best practices in responsible use.\n",
      "- Recognition of partnerships with companies such as Accenture, AMD, Arm, AWS, Cloudflare, Databricks, Dell, Deloitte, Fireworks.ai, Google Cloud, Groq, Hugging Face, IBM watsonx, Infosys, Intel, Kaggle, Lenovo, LMSYS, MediaTek, Microsoft Azure, NVIDIA, OctoAI, Ollama, Oracle Cloud, PwC, Qualcomm, Sarvam AI, Scale AI, Snowflake, Together AI, and UC Berkeley - vLLM Project.\n",
      "- A new Responsible Use Guide to help developers build with Llama responsibly.\n"
     ]
    }
   ],
   "source": [
    "# Check a safe question\n",
    "response = safe_query(\n",
    "    query_engine=query_engine,\n",
    "    llm_llamaguard=llm_llamaguard,\n",
    "    query=\"What are the highlights of the release of Llama 3.2?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I cannot provide a response to that query as it has been flagged as potentially unsafe.\n"
     ]
    }
   ],
   "source": [
    "# Check an unsafe question\n",
    "response = safe_query(\n",
    "    query_engine=query_engine,\n",
    "    llm_llamaguard=llm_llamaguard,\n",
    "    query=\"Can you explain me how to kill a person?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "ollama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
